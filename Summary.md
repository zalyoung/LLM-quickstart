### 毕业总结

在为期两个月的《AI大模型微调训练营》学习过程中，自己深刻体会了 AI 大模型在现代科技中的巨大潜力，并通过全面和深入的学习，初步掌握了大模型主流技术的原理和实战应用。课程从 AI 浪潮开始讲起，开篇通过提示工程、AI智能体、大模型微调、预训练技术四阶技术的基本介绍，让我对当前与大模型相关的核心技术有一个相对全面的了解。

而微调作为此次训练营的核心，也是自己最大的收获————在于对大模型微调技术的全面理解和实际操作经验的积累。通过学习Parameter-Efficient Fine-Tuning (PEFT)、Adapter Tuning、LoRA、QLoRA等前沿微调方法，我不仅在彭老师通俗易通的讲解下，掌握了相关的理论知识，还在 Homework 中成功应用这些技术对不同的模型进行了实际操作。整个过程中也让我对 Hugging Face 有了更加全面的了解，以前只是单纯的认为是一个代码、模型和数据集的托管平台，现在才认识到 transformers 相关的库才是其真正的核心价值所在。

在实操过程中，QLoRA 技术给我留下了深刻的印象。通过 QLoRA 技术，我们能够在保持模型性能的前提下，大幅减少 GPU 资源的消耗，采用老师推荐的 16GB 的 T4 基本能够完成所有实操课程。在基于私有数据微调 ChatGLM-6B 的过程中，也是我着重想要去了解和学习的内容，毕竟这个和实际的工作贴的更近。通过QLoRA，我们能够更高效地加载和处理大规模数据集，这极大地提高了训练速度同时也降低了对计算资源的需求。QLoRA 在处理特定领域数据时表现得非常出色，使得微调后的模型能够更准确地理解和生成与领域相关的内容。这一过程中，我不仅掌握了数据获取、增强和加载等技术细节，还学会了如何利用 QLoRA 优化模型参数，提升模型的适应性和响应能力。这对于我在不久的将来，推动 AI 技术在实际项目中的应用与创新尤为重要。

最后，特别感谢训练营的讲师团队，尤其是彭老师的专业性，以及娓娓道来的讲述方式和贴近生活的例子，让枯燥的论文内容和技术理论更容易被理解，同时也增加了很多记忆点。关于这门课程，一遍学习也是远远不够的，好在可以永久观看，期望随着自己在工作中实践更加深入，未来能有更多机会参与到类似的高水平培训项目中，不断提升自我，与大家共同进步。

